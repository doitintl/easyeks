import { Easy_EKS_Config_Data } from '../../lib/Easy_EKS_Config_Data';
import { Easy_EKS_Dynamic_Config } from '../../lib/Easy_EKS_Dynamic_Config';
import * as cdk from 'aws-cdk-lib';
import * as eks from 'aws-cdk-lib/aws-eks'
import * as ec2 from 'aws-cdk-lib/aws-ec2'
import {
    Grafana_Input_Parameters,
    Prometheus_Input_Parameters,
    Quickwit_Input_Parameters,
    Vector_Input_Parameters,
    Grafana_Prometheus_Quickwit_Vector,
} from '../../lib/GPQV_Observability';
import { 
    CloudWatch_Metrics_Input_Parameters,
    CloudWatch_Logs_Input_Parameters,
    CloudWatch_Metrics_and_Logs_Observability
} from '../../lib/CW_Observability/CW_Observability';
import {
    Apply_Podinfo_Helm_Chart,
    Apply_Podinfo_Http_Alb_YAML,
    Apply_Podinfo_Https_Alb_YAML,
    Podinfo_Helm_Config,
} from '../../lib/Podinfo_Manifests';

//Intended Use: 
//EasyEKS Admins: edit this file with config to apply to all dev / sandbox cluster's in your org.

export function apply_config(config: Easy_EKS_Config_Data, stack: cdk.Stack) { //config: is of type Easy_EKS_Config_Data
    config.add_tag("Environment", "Dev");
}//end apply_config()

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

export function deploy_addons(config: Easy_EKS_Config_Data, stack: cdk.Stack, cluster: eks.Cluster) {

    /////////////////////////////////////////////////////////////////////////////////////////////////////
    // v-- most won't need this, so commented out by default
    // const csi_snapshot_controller_for_kube_PVCs = new eks.CfnAddon(stack, 'snapshot-controller', {
    //     clusterName: cluster.clusterName,
    //     addonName: 'snapshot-controller',
    //     addonVersion: Easy_EKS_Dynamic_Config.get_latest_version_of_snapshot_controller_eks_addon(), //OR 'v8.3.0-eksbuild.1'
    //     resolveConflicts: 'OVERWRITE',
    //     configurationValues: '{}',
    // });
    /////////////////////////////////////////////////////////////////////////////////////////////////////

}//end deploy_addons()

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

export function deploy_essentials(config: Easy_EKS_Config_Data, stack: cdk.Stack, cluster: eks.ICluster){

    /////////////////////////////////////////////////////////////////////////////////////////////////////
    //config.CW.deploy_configured_cloudwatch_metrics_observability(config);
    // ^--Uncommenting the one line directly above:
    //    * Enables Metrics via: CloudWatch -> Container Insights
    //    * Deploys in namespace amazon-cloudwatch
    //      * daemonset cloudwatch-agent
    //      * replicaset amazon-cloudwatch-observability-controller-manager
    /////////////////////////////////////////////////////////////////////////////////////////////////////
    //config.CW.deploy_configured_cloudwatch_logs_observability(config);
    // ^--Uncommenting the one line directly above:
    //    * Enables Logs via: CloudWatch -> Log Insights to query container, pod, contatainerd, and kubelet logs
    //      You'll see 2 log groups: (a 3rd named host isn't seen, because it's not relevant to bottlerocket AMI)
    //      1. /aws/containerinsights/$CLUSTER_NAME/application  (container logs generated by pods)
    //      2. /aws/containerinsights/$CLUSTER_NAME/dataplane    (containerd & kubelet logs)
    //    * Deploys in namespace amazon-cloudwatch
    //      * daemonset fluent-bit 
    /////////////////////////////////////////////////////////////////////////////////////////////////////

    const quickwit_input:Quickwit_Input_Parameters = {
        enabled: true,
        rds_instance_type: ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE4_GRAVITON, ec2.InstanceSize.MICRO), //t4g.micro
    };
    const vector_input:Vector_Input_Parameters = {
        enabled: false,
    };
    const prometheus_input:Prometheus_Input_Parameters = {
        enabled: false,
    }
    const grafana_input:Grafana_Input_Parameters = {
        enabled: false,
    }
    config.GPQV.set_input_parameters_of_quickwit(quickwit_input);
    config.GPQV.set_input_parameters_of_vector(vector_input);
    config.GPQV.set_input_parameters_of_prometheus(prometheus_input);
    config.GPQV.set_input_parameters_of_grafana(grafana_input);
    config.GPQV.deploy_configured_GPQV_Observability_Stack(config);

}//end deploy_essentials()

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

export function deploy_workloads(config: Easy_EKS_Config_Data, stack: cdk.Stack, cluster: eks.ICluster) {

    // This is an example of a workload that uses a PersistentVolumeClaim with a storage class that is encrypted
    // with AWS KMS key.
    // IMPORTANT: if the cdk insfrastructure is destroyed it will leave the volume orphans, and they will
    // need to be manually deleted.
    let name="test-claim-gp3";
    let size="10Gi";
    const volume_claim_gp3 = {
        "apiVersion": "v1",
        "kind": "PersistentVolumeClaim",
        "metadata": {
            "name": `${name}`,
            "namespace": "default"
        },
        "spec": {
            "accessModes": [
                "ReadWriteOnce"
            ],
            "storageClassName": "kms-encrypted-gp3",
            "resources": {
                "requests": {
                    "storage": `${size}`
                }
            }
        }
    }
    const pod_using_volume_claim = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": "app"
        },
        "spec": {
            "containers": [
                {
                    "name": "app",
                    "image": "ubuntu:latest",
                    "command": [
                        "/bin/sh"
                    ],
                    "args": [
                        "-c",
                        "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"
                    ],
                    "volumeMounts": [
                        {
                            "name": "persistent-storage",
                            "mountPath": "/data"
                        }
                    ]
                }
            ],
            "volumes": [
                {
                    "name": "persistent-storage",
                    "persistentVolumeClaim": {
                        "claimName": `${name}`
                    }
                }
            ]
        }
    }
    const pvc_demo_construct = new eks.KubernetesManifest(stack, "persistentVolumeClaimManifest",
    {
        cluster: cluster,
        manifest: [volume_claim_gp3, pod_using_volume_claim],
        overwrite: true,
        prune: true,
    });






  // Define a BLUE podinfo application with insecure ALB (HTTP)
  const BLUE_PODINFO_HELM_CONFIG = {
    helm_chart_release: "podinfo-blue",
    helm_chart_values: {
      ui: {
        color: "#0000FF",
        message: "This is an insecure application with BLUE background",
      },
    } as Record<string, any>,
  } as Podinfo_Helm_Config

  // Deploy a podinfo sample application with BLUE background
  // Apply_Podinfo_Helm_Chart(cluster, config, stack, BLUE_PODINFO_HELM_CONFIG);

  // Provisioning HTTP ALB, which includes HTTP ALB, Listener, Target Group, etc.
  // Apply_Podinfo_Http_Alb_YAML(cluster, config, stack, BLUE_PODINFO_HELM_CONFIG)

  // Define a GREEN podinfo application with secure ALB (HTTPS)
  const GREEN_PODINFO_HELM_CONFIG = {
    helm_chart_release: "podinfo-green",
    helm_chart_values: {
      ui: {
        color: "#008000",
        message: "This is an secure application with GREEN background",
      },
    } as Record<string, any>,
  } as Podinfo_Helm_Config

  // Deploy a podinfo sample application with GREEN background
  // Apply_Podinfo_Helm_Chart(cluster, config, stack, GREEN_PODINFO_HELM_CONFIG);

  // Generate HTTPS ingress manifest
  /**
   * TODO: due to DNS ACME challenge, we just use the existing ACME's ARN and subdomain
   * To make this happen, you need to do:
   * 1. Prepare a domain or sub-domain
   * 2. Create a certificate in ACM for the domain / sub-domain
   * 3. Create CNAME to verify the certificate successfully
   * 4. Get the ARN of the certificate
   * 5. Deploy the stack
   * 6. After ALB is provisioned, create a CNAME record of the domain/sub-domain with the value in the DNS hostname of the ALB
   */
  // const https_ingress_yaml = Podinfo_Https_Ingress_Yaml_Generator(
  //   GREEN_PODINFO_HELM_CONFIG,
  //   // ACME ARN
  //   "arn:aws:acm:ap-southeast-2:092464092456:certificate/a2e016d5-58fb-4308-b894-f7a21f7df0b8",
  //   // Sub-domain
  //   "kefeng-easyeks.gcp.au-pod-1.cs.doit-playgrounds.dev",
  // )

  // kubectl apply manifest
//   Apply_Podinfo_Https_Alb_YAML(cluster, config, stack,
//     GREEN_PODINFO_HELM_CONFIG,
//     "arn:aws:acm:ap-southeast-2:092464092456:certificate/a2e016d5-58fb-4308-b894-f7a21f7df0b8")
}//end deploy_workloads()
